

![image](https://github.com/user-attachments/assets/5bbc84f5-5fed-47de-a08c-9f5ac7db3164)

# Bash-Linux-Troubleshoot-and-Automation-Toolkit

## Overview

Effective Linux system administration and cybersecurity necessitate prompt diagnostics, comprehensive visibility, and intelligent automation. This repository consists of a collection of Bash scripts designed to empower Linux administrators, DevOps engineers, and security teams with immediate troubleshooting capabilities. These scripts are intended to capture system metrics, diagnose issues, automate health checks, and enhance operational awareness, all while employing standard Linux utilities without the need for external dependencies.

This toolkit is particularly suitable for system administrators managing Linux servers, professionals engaged in learning Linux system administration, and experienced IT professionals seeking lightweight and reliable utilities for live incident response or root cause analysis. Each script is modular, well-documented, parameterizable, and is safe for utilization across various Linux distributions on servers, workstations, or virtual machines.

This repository delivers a curated **suite of 12 Bash scripts** that empower system administrators, DevOps engineers, and security responders to:

* ğŸ” **Capture critical evidence** within seconds.
* ğŸ› ï¸ **Pinpoint misconfigurations** before they snowball.
* ğŸ¤– **Automate routine hygiene tasks** so you can focus on higherâ€‘value work.

Each script is:

* **Dependencyâ€‘free** â€“ built only on ubiquitous GNU/Linux tooling.
* **Parameterâ€‘driven** â€“ easily tailored to your environment.
* **Commentâ€‘rich** â€“ ready for learning, audits, and pullâ€‘requests.

Whether you are a junior engineer maintaining your first VPS, a blueâ€‘team analyst on an IR bridge call, or a hiring manager skimming for actionable skills, this toolkit showcases **productionâ€‘safe Bash craftsmanship**.

<details>
<summary><strong>ğŸ“š Table of ContentsÂ â€” click to expand</strong></summary>

* [Quickâ€‘Start](#quickâ€‘start)
* [Scriptâ€‘Catalogue](#scriptâ€‘catalogue)

  1. [collect\_system\_logs.sh](#1ï¸âƒ£-collect_system_logssh)
  2. [check\_system\_integrity.sh](#2ï¸âƒ£-check_system_integritysh)
  3. [get\_active\_connections.sh](#3ï¸âƒ£-get_active_connectionssh)
  4. [system\_health\_snapshot.sh](#4ï¸âƒ£-system_health_snapshotsh)
  5. [detect\_bruteforce\_logons.sh](#5ï¸âƒ£-detect_bruteforce_logonssh)
  6. [get\_listening\_ports.sh](#6ï¸âƒ£-get_listening_portssh)
  7. [audit\_local\_admin\_members.sh](#7ï¸âƒ£-audit_local_admin_memberssh)
  8. [run\_antivirus\_scan.sh](#8ï¸âƒ£-run_antivirus_scansh)
  9. [test\_network\_connectivity.sh](#9ï¸âƒ£-test_network_connectivitysh)
  10. [export\_firewall\_rules.sh](#ğŸ”Ÿ-export_firewall_rulessh)
  11. [schedule\_automated\_system\_updates.sh](#1ï¸âƒ£1ï¸âƒ£-schedule_automated_system_updatessh)
  12. [rotate\_and\_archive\_logs.sh](#1ï¸âƒ£2ï¸âƒ£-rotate_and_archive_logssh)
* [Conclusion](#conclusion)

</details>

---

## Guide
### ğŸ“š Quickâ€‘Start-Guide

To get started with the toolkit, simply clone the repository to your local machine using git. This allows you to immediately access all of the included scripts. Once downloaded, navigate into the directory and make the scripts executable using chmod. You can then run any script directly from your terminal by calling its filename with ./. 

These scripts are designed to be portable and require no external libraries, making them ideal for environments with limited internet access or tight security controls. They are suitable for laptops, servers, and virtual machines. Most scripts include comments and examples to help new users understand their purpose and structure.

```bash
# 1. Clone the repo
 git clone https://github.com/<yourâ€‘org>/bash-linux-troubleshoot-toolkit.git
 cd bash-linux-troubleshoot-toolkit

# 2. Make everything executable
 chmod +x Scripts/*.sh

# 3. Run any helper â€“ e.g. grab the last 12Â h of syslog + auth
 ./Scripts/collect_system_logs.sh --hours 12 --logs syslog,auth --output /tmp/IR
```

All helpers accept `-h | --help` and print inline docs.


---
## Scriptâ€‘Catalogue

### 1ï¸âƒ£Â collect\_system\_logs.sh

In high-pressure incidents, every second counts, and the phrase "Logs or it did not happen" rings true. Navigating complex log files can lead to errors and delays, making collect_system_logs.sh an essential tool. This script automates the export of Syslog and journalctl files, organizing raw data into a compressed, forensics-ready archive.

Instead of juggling multiple commands, users can access a structured archive suitable for immediate SIEM import or ticket attachment. The script categorizes log sources into distinct, timestamped files (e.g., syslog.log.gz, auth.log.gz), allowing analysts to quickly search logs with zgrep, input clean data into tools like Splunk, and compare collections to identify critical changes.

#### HowÂ itÂ works

* Accepts `--hours`, `--logs` (commaâ€‘separated), `--journal_unit`, and `--output`.
* Autoâ€‘creates the destination folder.
* For classic files it uses an **awk date filter** (no brittle `sed` hacks).
* For systemd hosts it calls `journalctl --since` with `--output export`.
* Compresses each log with `gzip â€‘9` to save bandwidth when copying.

#### UsageÂ Example

```bash
# Last 24Â h of syslog + auth
./collect_system_logs.sh --hours 24 --logs syslog,auth --output /var/tmp/incident_logs

# Everything SSHD wrote in the past 2Â days
./collect_system_logs.sh --journal_unit sshd --since "2 days ago" --output /var/tmp/ssh_logs
```

```bash
#!/usr/bin/env bash
set -euo pipefail
HOURS=24; LOGS="syslog"; OUT="$(pwd)/Logs"; UNIT=""; SINCE=""
while [[ $# -gt 0 ]]; do
  case $1 in
    --hours)   HOURS="$2" ; shift 2;;
    --logs)    LOGS="$2"  ; shift 2;;
    --output)  OUT="$2"   ; shift 2;;
    --journal_unit) UNIT="$2"; shift 2;;
    --since)   SINCE="$2" ; shift 2;;
    -h|--help) grep -E "^#" "$0" | cut -c4-; exit 0;;
    *) echo "Unknown: $1"; exit 1;;
  esac;
done
mkdir -p "$OUT"
if [[ -n $UNIT ]]; then
  journalctl -u "$UNIT" --since "${SINCE:-$HOURS hour ago}" | gzip > "$OUT/${UNIT}.journal.gz"
  echo "âœ“ exported journal for $UNIT"
else
  START="$(date --date="-$HOURS hour" '+%b %e %H:%M:%S')"
  IFS=',' read -ra FILES <<< "$LOGS"
  for f in "${FILES[@]}"; do
    [[ -f /var/log/$f ]] || { echo "! /var/log/$f missing"; continue; }
    awk -v dt="$START" '$0>=dt' "/var/log/$f" | gzip > "$OUT/${f}.log.gz"
    echo "âœ“ exported $f"
  done
fi
```

---

### 2ï¸âƒ£Â check\_system\_integrity.sh

Package or file integrity drift often precedes outages and breaches. This script unifies rpm -Va and debsums checks, drops results into a timestamped report, and highlights unexpected hashes or permissions so you can intervene before minor deviations become service stopping incidents.

Beyond vendor packages, the tool can be extended to snapshot hashes of bespoke configuration files such as /etc/ssh/sshd_config, providing an extra guardrail against silent backdoors or well intentioned but undocumented changes. Pair it with a nightly cron to create a tamper evident audit trail.


#### HowÂ itÂ works

* Detects distro via `/etc/os-release`.
* If Debianâ€‘based: ensures `debsums` is installed; else prompts.
* If RPMâ€‘based: runs `rpm -Va`.
* Saves anomalies to `integrity_YYYYMMDD.txt` in `$PWD`.
* Emits a simple verdict (`âœ” OK` vs `âš  Issues found`).

#### UsageÂ Example

```bash
sudo ./check_system_integrity.sh
```

```bash
#!/usr/bin/env bash
set -euo pipefail
LOG="integrity_$(date +%Y%m%d).txt"
source /etc/os-release
if [[ $ID =~ (debian|ubuntu) ]]; then
  command -v debsums >/dev/null || { echo "Install debsums: sudo apt install debsums"; exit 1; }
  debsums -s > "$LOG"
else
  rpm -Va | tee "$LOG"
fi
[[ -s $LOG ]] && echo "âš ï¸ Issues found â€“ see $LOG" || echo "âœ” Integrity OK"
```

---

### 3ï¸âƒ£Â get\_active\_connections.sh

Command and control beacons leave unmistakable footprints as ESTABLISHED sockets. This helper surfaces every outbound TCP session, correlating remote endpoints with the owning process and user account. The result is a sortable list ideal for rapid threat hunt filtering or bandwidth hog investigations.

For defenders, it spotlights suspicious destinations and uncovers covert data exfiltration. For operators, it quickly reveals runaway services or misconfigured daemons saturating network links without the overhead of full stack profilers.

> For threat hunts it highlights suspicious destinations; for ops it surfaces rogue processes hogging links.

#### HowÂ itÂ works

* Wraps `netstat -pant` (fallback: `ss -pant`).
* Filters on `ESTABLISHED` state.
* Parses PID/command and maps to username.
* Sorts by remote host for coherence.

#### UsageÂ Example

```bash
./get_active_connections.sh | grep -E "\b443\b"
```

```bash
#!/usr/bin/env bash
set -euo pipefail
sudo netstat -pant 2>/dev/null | awk '$6=="ESTABLISHED" {print $4,$5,$7}' | while read l r pu; do
  pid="${pu%%/*}"; proc="${pu##*/}"; user=$(ps -o user= -p "$pid" 2>/dev/null || echo "-")
  printf "%s -> %-22s %-8s %s\n" "$l" "$r" "$user" "$proc"
done | sort -k2
```

---

### 4ï¸âƒ£Â system\_health\_snapshot.sh

Every effective troubleshooting session starts with a baseline. This script captures CPU load averages, memory utilisation, disk capacity, and count of pending updates in one compact reportâ€”providing an immediate pulse check on host health. Store successive snapshots to quantify the impact of tuning or to satisfy change approval evidence requirements.

Because it relies solely on ubiquitous GNU utilities (uptime, free, df, and package managers), the script executes consistently across distributions and even minimal recovery environments, giving you dependable metrics no matter where you run it.

> Capture CPU load, memory, disk, and pending updates in one shot.

#### HowÂ itÂ works

* CPU: `uptime` for 1/5/15â€‘min load.
* Memory: `free -m`.
* Disk: `df -hT` sans tmpfs.
* Pending updates: autoâ€‘detects `apt`, `dnf`, or `yum`.
* Prints a labeled report.

#### UsageÂ Example

```bash
./system_health_snapshot.sh > before_fix.txt
# apply tuningâ€¦
./system_health_snapshot.sh > after_fix.txt
diff -u before_fix.txt after_fix.txt
```

```bash
#!/usr/bin/env bash
set -euo pipefail
echo "== CPU LOAD =="; uptime

echo "\n== MEMORY (MB) =="; free -m

echo "\n== DISK (GB) =="; df -hT | grep -Ev '^tmpfs|^udev'

if command -v apt >/dev/null; then
  echo "\n== PENDING APT UPDATES =="; sudo apt list --upgradable 2>/dev/null | tail -n +2 | wc -l
elif command -v dnf >/dev/null; then
  echo "\n== PENDING DNF UPDATES =="; sudo dnf check-update -q | grep -c '^'
fi
```

---

### 5ï¸âƒ£Â detect\_bruteforce\_logons.sh

SSH brute force attempts flood logs long before a compromise. This analyser parses authentication records for failed passwords, grouping attempts by IP and user, then flags offenders that exceed a configurable threshold. Feed the output directly into fail2ban, firewall blocklists, or SIEM watchlists for real time protection.

Historical reports also reveal attack trends, helping security teams justify MFA roll outs and hardening budgets with concrete data instead of anecdotal evidence.


#### HowÂ itÂ works

* Accepts `--hours` and `--threshold`.
* Greps `/var/log/auth.log` *or* journal for "Failed password".
* Parses IP + username with `awk`.
* Uses `uniq -c` to tally.
* Prints offenders sorted highâ€‘toâ€‘low.

#### UsageÂ Example

```bash
sudo ./detect_bruteforce_logons.sh --hours 12 --threshold 20
```

```bash
#!/usr/bin/env bash
set -euo pipefail
HOURS=24; THRESH=15
while [[ $# -gt 0 ]]; do
  case $1 in
    --hours) HOURS="$2"; shift 2;;
    --threshold) THRESH="$2"; shift 2;;
    *) echo "Usage: $0 [--hours N] [--threshold N]"; exit 1;;
  esac
done
SINCE="$(date --date="-$HOURS hour" '+%b %e %H')"
LOGSRC="/var/log/auth.log"
[[ -f $LOGSRC ]] || { echo "No $LOGSRC found"; exit 1; }
awk -v dt="$SINCE" '$0>dt && /Failed password/' "$LOGSRC" | \
  awk '{ip=$NF; user=$(NF-5); print ip","user}' | sort | uniq -c | sort -nr | \
  awk -v t=$THRESH '$1>=t {printf "%s attempts\tIP:%s\tUser:%s\n",$1,substr($2,1,index($2,",")-1),substr($2,index($2,",")+1)}'
```

---

### 6ï¸âƒ£Â get\_listening\_ports.sh

Unintended listening services are low hanging fruit for attackers. This utility enumerates every TCP and UDP socket in a LISTEN state, tagging each with protocol, address, port, PID, and executable name. That clarity allows administrators to spot rogue backdoors or verify that micro services expose only authorised interfaces.

Embed it in CI smoke tests or golden image validation to enforce network surface standards automaticallyâ€”no manual port audits required.

> Map every service bound to a port, crucial for detecting rogue listeners or confirming app binds

#### HowÂ itÂ works

* Executes `ss -tulnp` (fallback: `netstat -tulnp`).
* Extracts protocol, address\:port, PID.
* Resolves PID â†’ command via `/proc/<pid>/comm`.
* Sorts by address.

#### UsageÂ Example

```bash
./get_listening_ports.sh | grep 8080
```

```bash
#!/usr/bin/env bash
set -euo pipefail
sudo ss -tulnp | awk 'NR>1 {print $1,$5,$7}' | while read proto addr proc; do
  pid="${proc%%,*}"; pid="${pid//pid=}"; cmd="$(ps -p $pid -o comm= 2>/dev/null)";
  printf "%-4s %-25s %-6s\n" "$proto" "$addr" "$cmd"
done | sort -k2
```

---

### 7ï¸âƒ£Â audit\_local\_admin\_members.sh

Privilege sprawl erodes security posture. This script cross checks UID 0 accounts and members of privileged groups (sudo, wheel) against a defined safe list, immediately flagging unauthorised admin access. Run it periodically and ship the diff to your SIEM for an iron clad least privilege audit.

Automation here replaces error prone manual reviews and acts as an early warning system for compromised or leftover accounts following personnel changes.


#### HowÂ itÂ works

* UIDÂ 0 accounts from `/etc/passwd`.
* Group parsing via `getent group`.
* Compares against hardâ€‘coded SAFE array (edit to suit).
* Prints âœ… expected vs âš  review.

#### UsageÂ Example

```bash
./audit_local_admin_members.sh | grep âš 
```

```bash
#!/usr/bin/env bash
set -euo pipefail
SAFE=(root adminuser)
admins=( $(awk -F: '($3==0){print $1}' /etc/passwd) )
for g in sudo wheel; do
  [[ $(getent group $g) ]] && admins+=( $(getent group $g | cut -d: -f4 | tr , ' ') )
done
for u in "${admins[@]}"; do
  if [[ " ${SAFE[*]} " =~ " $u " ]]; then
    echo "âœ” $u (expected)"
  else
    echo "âš  $u (review)"
  fi
done | sort -u
```

---

### 8ï¸âƒ£Â run\_antivirus\_scan.sh

Contrary to myth, Linux malware existsâ€”and unchecked file servers can become propagation hubs. This wrapper triggers a recursive, low noise ClamAV scan, logs findings, and summarises infections in a single line so health monitors or cron mail can pick them up instantly.

Use it to vet user uploads, sweep build artefacts before CI promotion, or validate cloud images pre deployment without succumbing to heavier commercial agents.

> Invoke **ClamAV** (or swap for your engine) on demand, then parse a clean summary.

#### HowÂ itÂ works

* Checks for `clamscan` binary.
* Accepts optional target path; defaults to `/`.
* Logs to timestamped file.
* Evaluates infection count and prints verdict.

#### UsageÂ Example

```bash
sudo ./run_antivirus_scan.sh /var/www
```

```bash
#!/usr/bin/env bash
set -euo pipefail
TARGET=${1:-/}
LOG="clamscan_$(date +%Y%m%d_%H%M).log"
command -v clamscan >/dev/null || { echo "Install ClamAV"; exit 1; }
clamscan -ri "$TARGET" | tee "$LOG"
if grep -q "Infected files: 0" "$LOG"; then
  echo "âœ” No threats detected"
else
  echo "âš  Threats found â€“ see $LOG"
fi
```

---

### 9ï¸âƒ£Â test\_network\_connectivity.sh

Contrary to myth, Linux malware existsâ€”and unchecked file servers can become propagation hubs. This wrapper triggers a recursive, low noise ClamAV scan, logs findings, and summarises infections in a single line so health monitors or cron mail can pick them up instantly.

Use it to vet user uploads, sweep build artefacts before CI promotion, or validate cloud images pre deployment without succumbing to heavier commercial agents.

> Differentiate local, LAN, or WAN problems using a combo of `ping` and `traceroute`/`mtr`.

#### HowÂ itÂ works

* Builtâ€‘in TARGETS array (edit) or pass args.
* Pings with 4 probes + 2â€‘sec timeout.
* If unreachable, runs `traceroute -n` up to 20 hops.
* Prints reachability, RTT, and last hop.

#### UsageÂ Example

```bash
./test_network_connectivity.sh github.com 8.8.8.8
```

```bash
#!/usr/bin/env bash
set -euo pipefail
TARGETS=("$@")
[[ ${#TARGETS[@]} -eq 0 ]] && TARGETS=(8.8.8.8 1.1.1.1 github.com)
for t in "${TARGETS[@]}"; do
  if ping -c4 -W2 "$t" &>/dev/null; then
    rtt=$(ping -c4 "$t" | awk -F/ 'END{print $5" ms"}')
    echo "âœ” $t reachable â€“ avg RTT $rtt"
  else
    hops=$(traceroute -n -m 20 "$t" | tail -1 | awk '{print $1}')
    echo "âœ– $t unreachable â€“ reached $hops hops"
  fi
done
```

---

### ğŸ”ŸÂ export\_firewall\_rules.sh

Firewall rules drift over time through updates, hot fixes, and human edits. This helper snapshots the current iptables or nftables ruleset to a timestamped file for backup, peer review, or compliance archivesâ€”ensuring you can always roll back or audit historical changes without combing through shell history.
Automate it post deployment so every change to infrastructure has an accompanying rule export under version control.

> Track rule drift by exporting iptables/nftables to a file you can restore or audit.

#### HowÂ itÂ works

* Checks for `iptables-save`, else `nft`.
* Emits to `firewall_YYYYMMDD_HHMM.rules`.

#### UsageÂ Example

```bash
sudo ./export_firewall_rules.sh
```

```bash
#!/usr/bin/env bash
set -euo pipefail
OUT="firewall_$(date +%Y%m%d_%H%M).rules"
if command -v iptables-save >/dev/null; then
  sudo iptables-save > "$OUT"
elif command -v nft >/dev/null; then
  sudo nft list ruleset > "$OUT"
else
  echo "No supported firewall found"; exit 1;
fi
echo "âœ“ Rules saved to $OUT"
```

---

### 1ï¸âƒ£1ï¸âƒ£Â schedule\_automated\_system\_updates.sh

Timely patching thwarts the majority of commodity exploits. This script installs a root owned weekly cron job that runs unattended upgrades with full logging, turning patch management from an intermittent task into a set and forget baseline.
Coupled with log shipping, it yields a verifiable patch compliance trail without the need for heavy management platforms on every host.

> Unpatched boxes invite 0â€‘days. This helper drops a rootâ€‘owned weekly cron that runs unattended updates and logs output.

#### HowÂ itÂ works

* Writes `/etc/cron.weekly/auto_update` with distroâ€‘aware commands.
* Logs to `/var/log/auto_update.log`.

#### UsageÂ Example

```bash
sudo ./schedule_automated_system_updates.sh
```

```bash
#!/usr/bin/env bash
set -euo pipefail
CRON="/etc/cron.weekly/auto_update"
cat <<'EOS' | sudo tee "$CRON" >/dev/null
#!/bin/bash
exec > /var/log/auto_update.log 2>&1
if command -v apt >/dev/null; then
  apt update && apt -y dist-upgrade
elif command -v dnf >/dev/null; then
  dnf -y upgrade --refresh
fi
EOS
sudo chmod +x "$CRON"
echo "âœ“ Weekly auto-update scheduled ($CRON)"
```

---

### 1ï¸âƒ£2ï¸âƒ£Â rotate\_and\_archive\_logs.sh
Log bloat can crash services faster than almost any other resource issue. This helper scans for files over 50â€¯MB in /var/log, compresses them into an archive directory, and safely truncates the originalsâ€”emulating logrotate where the utility is absent or misconfigured.
Ideal for containers or IoT devices with limited storage, the script keeps critical logs available while ensuring disks stay well below panic thresholds.

> Prevent log partitions from filling by compressing logs >Â 50Â MB and shipping them to an archive folder.

#### HowÂ itÂ works

* Default source `/var/log` and dest `/var/log/archive` (edit vars).
* Finds large files not already `.gz`.
* `gzip -c` to archive; truncates original with `: > file`.

#### UsageÂ Example

```bash
sudo ./rotate_and_archive_logs.sh
```

```bash
#!/usr/bin/env bash
set -euo pipefail
SRC=/var/log; DST=/var/log/archive; SZ=50M
mkdir -p "$DST"
find "$SRC" -type f -size +$SZ ! -name '*.gz' | while read f; do
  gzip -c "$f" > "$DST/$(basename "$f").gz" && : > "$f"
  echo "Rotated $f"
done
```

---
---

## Conclusion

Conclusion

The Bash-Linux-Troubleshoot Toolkit represents a practical and purpose-driven resource for professionals and learners who prioritize speed, precision, and simplicity in Linux administration. Each script is meticulously designed to aid users in understanding and promptly addressing underlying issues using reliable tools available on most Linux systems. This toolkit benefits a range of users, from junior engineers managing their inaugural server to security analysts addressing threats and IT managers seeking to empower their teams while streamlining the troubleshooting workflow.

Beyond its functional merits, this compilation is an educational tool and a subject for discussion. Recruiters can assess real-world scripting skills, mentors can leverage the scripts to elucidate best practices, and contributors can evolve the toolkit to adapt to changing infrastructural needs. The toolkit remains versatile and extensible by maintaining a focus on clear, concise, and actionable scripting.

Thank you for your engagement with the Bash-Linux-Troubleshoot Toolkit. We aspire that it becomes an essential component of your operational playbook and an inspiration for developing automation-first solutions. We welcome your feedback and collaboration to enhance this resource for the broader open-source and IT community.

â­ The future repository if you find this concept useful! As scripts are developed, issues with bugs or new features are raised. Happy scripting â€” and automate all the things! 


ğŸ” â€” Happy hunting and automateÂ *all* the things! 
